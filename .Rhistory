u_ = -v * c * log(u)
u_
f
qchisq(0.95, f)
p = dim(y_data)[2]
p
n_3 = dim(y_data)[1]
v= n_3 -1
y1 = c(35,35,40,10,6,20,35,35,35,30)
y2 = c(3.5,4.9,30,2.8,2.7,2.8,4.6,10.9,8.0,1.6)
y3 = c(2.8,2.7,4.38,3.21,2.73,2.81,2.88,2.90,3.28,3.20)
#which one do we use, is it similar to the last problem
length(y1)
#binding the data into a matrix
y_data = cbind(y1,y2,y3)
y_data
#creating the covariance matrix for the data
S_overall = cov(y_data)
#finding the u to put into u' statistic
u = (det(cov(y_data)))/(S_overall[1,1] * S_overall[2,2] * S_overall[3,3])
#creating the covariance matrix for the data
S_overall = cov(y_data)
S_overall
(det(cov(y_data)))
(S_overall[1,1] * S_overall[2,2] * S_overall[3,3])
#finding the u to put into u' statistic
u = (det(cov(y_data)))/(S_overall[1,1] * S_overall[2,2] * S_overall[3,3])
u
n_3 = dim(y_data)[1]
v= n_3 -1
p = dim(y_data)[2]
p
u_ = -(v- (1/6)* (2*p+5)) * log(u)
u_[1]
#we reject the null if u' > chi square with df of f where
#f is
f = (1/2) * p * (p-1)
f
qchisq(0.95,f)
u_ > qchisq(0.95,f)
u_ = -(v- (1/6)* (2*p+5)) * log(u)
u_[1]
qchisq(0.05,f)
u_ > qchisq(0.05,f)
qchisq(0.01,10)
qchisq(0.99,10)
qchisq(0.95,f)
data(iris)
ind <- sample(2, nrow(iris),
replace = TRUE,
prob = c(0.6, 0.4))
dim(iris)
dim(training)
training <- iris[ind==1,]
testing <- iris[ind==2,]
dim(training)
dim(testing)
dim(iris[sample(1:150,90),])
dim(iris[sample(1:150,60),])
dim(iris[sample(1:nrow(iris), nrow(iris)*0.60),])
linear <- lda(Species~., training)
# need to do parition plot
library(MASS)
linear <- lda(Species~., training)
# need to do parition plot
#decision boundary should be made with all variables,
linear
p1 <- predict(linear, training)$class
tab <- table(Predicted~p1, actual = training$Species)
p1 <- predict(linear, training)$class
tab <- table(Predicted~p1, actual = training$Species)
set.seed(123)
tab <- table(Predicted=p1, actual = training$Species)
tab
linear <- lda(Species~., training)
set.seed(123)
# need to do parition plot
#decision boundary should be made with all variables,
linear
partimat(Species~.,data = training )
p1 <- predict(linear, training)$class
tab <- table(Predicted=p1, actual = training$Species)
tab
tab <- table(Predicted = p2, Actualy = testing$Species)
tab1 <- table(Predicted = p2, Actual = testing$Species)
p2 <- predict(linear, testing)$class
tab1 <- table(Predicted = p2, Actual = testing$Species)
tab1
## PCA
library(ade4)
data(deug)
library(psych)
data(deug)
deug
library(devtools)
data(deug)
library(heplots)
install.packages('ade4')
## PCA
library(ade4)
data(deug)
ev = eigen((cova(a)))$values
eve = eigen(cov(a))$vectors
#can use both, just need to explain it
a= deug$tab
ev = eigen((cova(a)))$values
eve = eigen(cov(a))$vectors
ev = eigen((cov(a)))$values
eve = eigen(cov(a))$vectors
l1 =
#scree plot, the eigen vectors from 1 to the last one,
# the purpose of a scree plot is to choose a small PCA, know always that the decreasing trend. Last one is the smallest one,
#the first one can almost explain the mot out of the 9, at least 1/9
# first several components are very large and thus we use largest one,
#if it is a smooth decreasing, need ot find where the decreaisng trend is not too bad
#chooes first 4
val <-eigen(S)$values
df = read.csv("Blue-Data.csv")
setwd("C:/Users/wesle/OneDrive/Desktop/STA-160-Final")
df_green = read.csv("Green-Data.csv")
#R AND G COMPARISON
ggplot(df_green, aes(x=R, y=G, color = B), color = "green") +
geom_point() + scale_x_continuous(breaks=seq(0,255,25)) +
scale_y_continuous(breaks=seq(0,255,25))
library(ggplot2)
df_green = read.csv("Green-Data.csv")
#R AND G COMPARISON
ggplot(df_green, aes(x=R, y=G, color = B), color = "green") +
geom_point() + scale_x_continuous(breaks=seq(0,255,25)) +
scale_y_continuous(breaks=seq(0,255,25))
df_red = read.csv("Red-Data.csv")
#R AND G COMPARISON
ggplot(df_red, aes(x=R, y=G, color = B)) +
geom_point() + scale_x_continuous(breaks=seq(0,255,25)) +
scale_y_continuous(breaks=seq(0,255,25)) +
scale_color_gradient(low = "#ffcccb ", high = "#8b0000")
#R AND G COMPARISON
ggplot(df_red, aes(x=R, y=G, color = B)) +
geom_point() + scale_x_continuous(breaks=seq(0,255,25)) +
scale_y_continuous(breaks=seq(0,255,25)) +
scale_color_gradient(low = "#FFCCCB", high = "#8b0000")
#R AND G COMPARISON
ggplot(df_red, aes(x=R, y=G, color = B)) +
geom_point() + scale_x_continuous(breaks=seq(0,255,25)) +
scale_y_continuous(breaks=seq(0,255,25)) +
scale_color_gradient(low = "#FFCCCB", high = "#FF0000")
knitr::opts_chunk$set(echo = TRUE)
library(mlbench)
data(Glass)
Glass
Glass
Glass
res <- boxM(Glass[,1:9], Glass[, "Type"])
library(mlbench)
library(mvtnorm)
###Real
library(klaR)
library(psych)
library(MASS)
#library(ggord)
library(devtools)
library(heplots)
data(Glass)
Glass
?Glass
res <- boxM(Glass[,1:9], Glass[, "Type"])
res <- boxM(Glass[,1:8], Glass[, "Type"])
res
res <- boxM(Glass[,1:9], Glass[, "Type"])
res
summary(res)
Glass[,1:9]
res <- boxM(Glass[,1:9], Glass[, "Type"])
res
summary(res)
Glass[,1]
cov(Glass[,1])
var(Glass[,1])
res <- boxM(Glass[,1:9], Glass[, "Type"])
res
summary(res)
Sepal.Length
data(iris)
data(iris)
Sepal.Length
Glass
boxM(cbind(Rl, Na, Mg, Al, Si, K, Ca, Ba, Fe) ~ Type, data=Glass)
boxM(cbind(RI, Na, Mg, Al, Si, K, Ca, Ba, Fe) ~ Type, data=Glass)
boxM(cbind(RI, Na, Mg, Al, Si, K, Ca, Ba, Fe) ~ Type, data=Glass)
# Box's M Test (check condition for LDA)
# default method
res <- boxM(iris[, 1:4], iris[, "Species"])
res
summary(res)
res <- boxM(Glass[,1:9], Glass[, "Type"])
res
summary(res)
res <- boxM(cbind(RI, Na, Mg, Al, Si, K, Ca, Ba, Fe) ~ Type, data=Glass)
res
summary(res)
# Box's M Test (check condition for LDA)
# default method
res <- boxM(iris[, 1:4], iris[, "Species"])
res
summary(res)
res <- boxM(cbind(RI, Na, Mg, Al, Si, Ca, Ba, Fe) ~ Type, data=Glass)
res
summary(res)
res <- boxM(cbind(RI, Na, Mg, Al, Si, K, Ca, Ba, Fe) ~ Type, data=Glass)
res
summary(res)
ind <- sample(2, nrow(Glass),
replace = TRUE,
prob = c(0.6, 0.4))
training <- Glass[ind==1,]
testing <- Glass[ind==2,]
dim(iris)
dim(training)
dim(testing)
dim(Glass)
dim(training)
dim(testing)
ind <- sample(2, nrow(Glass),
replace = TRUE,
prob = c(0.7, 0.3))
training <- Glass[ind==1,]
testing <- Glass[ind==2,]
dim(Glass)
dim(training)
dim(testing)
training <- Glass[sample(1:214,150),]
testing <- Glass[sample(1:214,65),]
dim(Glass)
dim(training)
dim(testing)
training <- Glass[sample(1:214,150),]
testing <- Glass[sample(1:214,64),]
dim(Glass)
dim(training)
dim(testing)
set.seed(15522) #setting a seed to make the results reproducible
training <- Glass[sample(1:214,150),] #making training data
testing <- Glass[sample(1:214,64),] #making testing data
dim(Glass)
Quadratic <- QDA(Species~., training)
Quadratic <- qda(Species~., training)
Quadratic <- qda(Type~., training)
Type
Quadratic <- qda(Glass$Type~., training)
length(Glass$RI)
length(Glass$K)
length(Glass)
length(Glass$Na)
Quadratic <- qda(Glass$Type~., training)
Quadratic <- qda(Glass$Type~Glass$RI, training)
Quadratic <- qda(Glass$Type~Glass$RI +Glass$Na, training)
Quadratic <- qda(Glass$Type~Glass$RI +Glass$Na + Glass$Mg, training)
Quadratic <- qda(Glass$Type~Glass$RI +Glass$Na + Glass$Mg +Glass$Al, training)
Quadratic <- qda(Glass$Type~Glass$RI +Glass$Na + Glass$Mg +Glass$Al+Glass$Si, training)
Quadratic <- qda(Glass$Type~Glass$RI +Glass$Na + Glass$Mg +Glass$Al+Glass$Si+Glass$K, training)
Glass$K
Quadratic <- qda(Glass$Type~Glass$RI +Glass$Na + Glass$Mg +Glass$Al+Glass$Si+Glass$Ca, training)
Quadratic <- qda(Glass$Type~Glass$RI +Glass$Na + Glass$Mg +Glass$Al+Glass$Si+Glass$Ca+Glass$Ba, training)
Quadratic <- qda(Glass$Type~Glass$RI +Glass$Na + Glass$Mg +Glass$Al+Glass$Si+Glass$Ca+Glass$Fe, training)
Quadratic <- qda(Glass$Type~Glass$RI +Glass$Na + Glass$Mg +Glass$Al+Glass$Si+Glass$Ca, training)
Quadratic <- qda(Glass$Type~Glass$RI +Glass$Na + Glass$Mg +Glass$Al+Glass$Si+Glass$Ca+Glass$K, training)
Glass
install.packages("ggpubr")
corr(Glass)
cor(Glass)
pairs.panels(Glass)
Quadratic <- qda(Glass$Type~Glass$RI +Glass$Na + Glass$Mg +Glass$Al+Glass$Si+Glass$Ca+Glass$K, training)
data(PimaIndiansDiabetes)
PimaIndiansDiabetes
res <- boxM(PimaIndiansDiabetes[, 1:8], PimaIndiansDiabetes[,"diabetes"])
summary(res)
?PimaIndiansDiabetes
training <- PimaIndiansDiabetes[sample(1:768,538),] #making training data
testing <- PimaIndiansDiabetes[sample(1:214,230),] #making testing data
testing <- PimaIndiansDiabetes[sample(1:768,230),] #making testing data
Quadratic <- qda(PimaIndiansDiabetes$diabetes~., training)
Quadratic <- qda(diabetes~., training)
Quadratic <- qda(Type~., training)
training <- Glass[sample(1:214,150),] #making training data
testing <- Glass[sample(1:214,64),] #making testing data
pairs.panels(Glass)
pairs.panels(Glass)
dim(training)
dim(testing)
Quadratic <- qda(Type~., training)
training <- PimaIndiansDiabetes[sample(1:768,538),] #making training data
testing <- PimaIndiansDiabetes[sample(1:768,230),] #making testing data
Quadratic <- qda(diabetes~., training)
training <- PimaIndiansDiabetes[sample(1:768,538),] #making training data
testing <- PimaIndiansDiabetes[sample(1:768,230),] #making testing data
Quadratic <- qda(diabetes~., training)
partimat(diabetes~., data = training, method = "qda")
partimat(diabetes~., data = training, method = "qda")
PimaIndiansDiabetes
set.seed(15522) #setting a seed to make the results reproducible
training <- PimaIndiansDiabetes[sample(1:768,538),] #making training data
testing <- PimaIndiansDiabetes[sample(1:768,230),] #making testing data
Quadratic <- qda(diabetes~., training)
partimat(diabetes~., data = training, method = "qda")
p1 <- predict(Quadratic, training)$class
tab <- table(Predicted = p1, Actual = training$Diabetes)
tab <- table(Predicted = p1, Actual = training$diabetes)
tab
p2 <- predict(Quadratic, testing)$class
tab1 <- table(Predicted = p2, Actual = testing$diabetes)
tab1
sum(diag(tab1))/sum(tab1)
sum(diag(tab1))/sum(tab1)
error_rate = 1-sum(diag(tab1))/sum(tab1)
library(caret)
library(Metrics)
install.package("Metrics")
install.packages("Metrics")
library(Metrics)
recall = recall(actual = testing$diabetes,
predicted = p2)
print(recall)
training$diabetes
df_diabetes = PimaIndiansDiabetes
df_diabetes[df=="pos"] = 1
df_diabetes[df=="pos"] <- 1
df_diabetes[df=="pos"] <- "1"
df_diabetes[df_diabetes=="pos"] <- 1
df_diabetes
df_diabetes[df_diabetes=="pos"] <- "1"
df_diabetes
df_diabetes[df_diabetes=="pos"] <- "1"
df_diabetes
df_diabetes[df_diabetes=="pos"] <- 1
df_diabetes
df_diabetes = PimaIndiansDiabetes
df_diabetes$diabetes = as.character(df_diabetes$diabetes)
df_diabetes[df_diabetes=="pos"] <- 1
df_diabetes
df_diabetes[df_diabetes=="neg"] <- 0
df_diabetes
df_diabetes$diabetes = as.factor(df_diabetes$diabetes)
e
set.seed(15522) #setting a seed to make the results reproducible
training <- PimaIndiansDiabetes[sample(1:768,538),] #making training data
testing <- PimaIndiansDiabetes[sample(1:768,230),] #making testing data
Quadratic <- qda(diabetes~., training)
partimat(diabetes~., data = training, method = "qda")
p1 <- predict(Quadratic, training)$class
tab <- table(Predicted = p1, Actual = training$diabetes)
tab
p2 <- predict(Quadratic, testing)$class
tab1 <- table(Predicted = p2, Actual = testing$diabetes)
tab1
training <- df_diabetes[sample(1:768,538),] #making training data
testing <- df_diabetes[sample(1:768,230),] #making testing data
Quadratic <- qda(diabetes~., training)
partimat(diabetes~., data = training, method = "qda")
p1 <- predict(Quadratic, training)$class
tab <- table(Predicted = p1, Actual = training$diabetes)
tab
p2 <- predict(Quadratic, testing)$class
tab1 <- table(Predicted = p2, Actual = testing$diabetes)
tab1
recall = recall(actual = testing$diabetes,
predicted = p2)
print(recall)
p2
test$diabetes
testing$diabetes
library(Metrics)
recall = recall(actual = testing$diabetes,
predicted = p2)
print(recall)
df_diabetes = PimaIndiansDiabetes
set.seed(15522) #setting a seed to make the results reproducible
training <- df_diabetes[sample(1:768,538),] #making training data
testing <- df_diabetes[sample(1:768,230),] #making testing data
Quadratic <- qda(diabetes~., training)
partimat(diabetes~., data = training, method = "qda")
p1 <- predict(Quadratic, training)$class
tab <- table(Predicted = p1, Actual = training$diabetes)
tab
p2 <- predict(Quadratic, testing)$class
tab1 <- table(Predicted = p2, Actual = testing$diabetes)
tab1
recall_rate = 126/(126+16)
recall_rate
precision_rate = 126/(126+32)
precision_rate
accuracy_rate = sum(diag(tab1))/sum(tab1)
accuracy_rate
error_rate
#R AND G COMPARISON
par(mfrow = c(1, 3))
ggplot(df, aes(x=R, y=G, color = B)) +
geom_point() + scale_x_continuous(breaks=seq(0,255,25)) +
scale_y_continuous(breaks=seq(0,255,25)) + xlab("Red") + ylab("Green") +
ggtitle("2D Projection of Red Pixels vs Green Pixels for Blue MM") +
scale_color_gradient(low = "#0285F9", high = "#254368")
setwd("C:/Users/wesle/OneDrive/Desktop/STA-160-Final")
library(ggplot2)
df = read.csv("Blue-Data.csv")
ggplot(df, aes(x=R, y=G, color = B)) +
geom_point() + scale_x_continuous(breaks=seq(0,255,25)) +
scale_y_continuous(breaks=seq(0,255,25)) + xlab("Red") + ylab("Green") +
ggtitle("2D Projection of Red Pixels vs Green Pixels for Blue MM") +
scale_color_gradient(low = "#0285F9", high = "#254368")
#R AND B COMPARISON
ggplot(df, aes(x=R, y=B, color = G)) +
geom_point() + scale_x_continuous(breaks=seq(0,255,25)) +
scale_y_continuous(breaks=seq(0,255,25)) + xlab("Red") + ylab("Blue") +
ggtitle("2D Projection of Red Pixels vs Blue Pixels for Blue MM") +
scale_color_gradient(low = "#0285F9", high = "#254368")
require(gridExtra)
plot1 <- ggplot(df, aes(x=R, y=G, color = B)) +
geom_point() + scale_x_continuous(breaks=seq(0,255,25)) +
scale_y_continuous(breaks=seq(0,255,25)) + xlab("Red") + ylab("Green") +
ggtitle("2D Projection of Red Pixels vs Green Pixels for Blue MM") +
scale_color_gradient(low = "#0285F9", high = "#254368")
#R AND B COMPARISON
plot2 <- ggplot(df, aes(x=R, y=B, color = G)) +
geom_point() + scale_x_continuous(breaks=seq(0,255,25)) +
scale_y_continuous(breaks=seq(0,255,25)) + xlab("Red") + ylab("Blue") +
ggtitle("2D Projection of Red Pixels vs Blue Pixels for Blue MM") +
scale_color_gradient(low = "#0285F9", high = "#254368")
#G AND B COMPARISON
plot3 <- ggplot(df, aes(x=G, y=B, color = R)) +
geom_point() + scale_x_continuous(breaks=seq(0,255,25)) +
scale_y_continuous(breaks=seq(0,255,25))+ xlab("Green") + ylab("Blue") +
ggtitle("2D Projection of Green Pixels vs Blue Pixels for Blue MM") +
scale_color_gradient(low = "#0285F9", high = "#254368")
grid.arrange(plot1, plot2, plot3, ncol=3)
dev.new(width = 3000, height = 1500, unit = "px")
dev.new(width = 4000, height = 1500, unit = "px")
dev.new(width = 4000, height = 2000, unit = "px")
dev.new(width = 4000, height = 2500, unit = "px")
dev.new(width = 4500, height = 2500, unit = "px")
dev.new(width = 5000, height = 2500, unit = "px")
#R AND G COMPARISON
plot1 <- ggplot(df, aes(x=R, y=G, color = B)) +
geom_point() + scale_x_continuous(breaks=seq(0,255,25)) +
scale_y_continuous(breaks=seq(0,255,25)) + xlab("Red") + ylab("Green") +
ggtitle("2D Projection of Red Pixels vs Green Pixels for Blue MM") +
scale_color_gradient(low = "#0285F9", high = "#254368")
#R AND B COMPARISON
plot2 <- ggplot(df, aes(x=R, y=B, color = G)) +
geom_point() + scale_x_continuous(breaks=seq(0,255,25)) +
scale_y_continuous(breaks=seq(0,255,25)) + xlab("Red") + ylab("Blue") +
ggtitle("2D Projection of Red Pixels vs Blue Pixels for Blue MM") +
scale_color_gradient(low = "#0285F9", high = "#254368")
#G AND B COMPARISON
plot3 <- ggplot(df, aes(x=G, y=B, color = R)) +
geom_point() + scale_x_continuous(breaks=seq(0,255,25)) +
scale_y_continuous(breaks=seq(0,255,25))+ xlab("Green") + ylab("Blue") +
ggtitle("2D Projection of Green Pixels vs Blue Pixels for Blue MM") +
scale_color_gradient(low = "#0285F9", high = "#254368")
grid.arrange(plot1, plot2, plot3, ncol=3)
dev.new(width = 6000, height = 2500, unit = "px")
dev.new(width = 6000, height = 3000, unit = "px")
dev.new(width = 7000, height = 3000, unit = "px")
dev.new(width = 7000, height = 3000, unit = "px")
dev.new(width = 7000, height = 4000, unit = "px")
dev.new(width = 8000, height = 4000, unit = "px")
dev.new(width = 8000, height = 4000, unit = "px")
grid.arrange(plot1, plot2, plot3, ncol=3)
dev.new(width = 10000, height = 4000, unit = "px")
dev.new(width = 10000, height = 5000, unit = "px")
#dev.new(width = 10000, height = 5000, unit = "px")
dev.new(width=5, height=4)
#dev.new(width = 10000, height = 5000, unit = "px")
dev.new(width=20, height=4)
#dev.new(width = 10000, height = 5000, unit = "px")
dev.new(width=25, height=4)
#dev.new(width = 10000, height = 5000, unit = "px")
dev.new(width=25, height=10)
#dev.new(width = 10000, height = 5000, unit = "px")
dev.new(width=25, height=15)
#dev.new(width = 10000, height = 5000, unit = "px")
dev.new(width=30, height=15)
library(infotheo)
df <- read.csv("Blue-Data.csv")
df <- df[,-c(1:2)]
varpars <- expand.grid(names(df),names(df))
for (i in 1:nrow(varpars)) {
varpars[i,3] <- condentropy(X = HDD[[varpars[i,1]]], Y = HDD[[varpars[i,2]]])
}
for (i in 1:nrow(varpars)) {
varpars[i,3] <- condentropy(X = df[[varpars[i,1]]], Y = df[[varpars[i,2]]])
}
names(varpars) <-c('X','Y','Cond_Entropy')
MTX <- reshape(varpars, idvar = "X", timevar = "Y", direction = "wide")
names(MTX)[2:4] <- names(df)
rownames(MTX) <- names(df)
MTX <- MTX[,-1]
MTX
df <- read.csv("Blue-Data.csv")
df <- df[,-c(1:2)]
varpars <- expand.grid(names(df),names(df))
for (i in 1:nrow(varpars)) {
varpars[i,3] <- condentropy(X = df[[varpars[i,1]]], Y = df[[varpars[i,2]]])
}
names(varpars) <-c('X','Y','Cond_Entropy')
MTX <- reshape(varpars, idvar = "X", timevar = "Y", direction = "wide")
names(MTX)[2:4] <- names(df)
rownames(MTX) <- names(df)
MTX <- MTX[,-1]
MTX
recall_rate
precision_rate
error_rate = 1-sum(diag(tab1))/sum(tab1)
error_rate
